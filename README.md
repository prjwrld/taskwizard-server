![Made with FastAPI](https://img.shields.io/badge/Made%20with-FastAPI-00c7b7?style=for-the-badge&logo=fastapi)
# TaskWizard Server âœ¨

Your personal task breakdown assistant, powered by **FastAPI** and a local **LLM** (Llama3.2 via Ollama).

## ğŸ› ï¸ Features
- ğŸš€ FastAPI backend
- ğŸ”¥ Local LLM (Ollama) integration
- ğŸ§  Smart prompt engineering (goal â†’ steps)
- ğŸŒ Web frontend (React Native + Expo exported)
- ğŸŒŸ 100% Offline or Local server running
- ğŸ“¦ Deployable to Render / Vercel / your own server

## ğŸ“‚ Project Structure

taskwizard-server/
â”œâ”€â”€ main.py         # FastAPI server logic
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md

## âš™ï¸ Requirements
- Python 3.10+ âœ…
- Ollama (local LLM runner) âœ…
- FastAPI, Uvicorn, Requests

## ğŸš€ Quick Start
```bash
# Clone the repo
git clone https://github.com/prjwrld/taskwizard-server.git
cd taskwizard-server

# Install dependencies
pip install -r requirements.txt

# Start Ollama (make sure it's running)
ollama run llama3.2

# Run the FastAPI server
uvicorn main:app --reload --port 8000

	â€¢	Visit: http://localhost:8000/docs (Swagger UI to test)

ğŸ† Credits
	â€¢	FastAPI
	â€¢	Ollama
	â€¢	Llama 3.2B Model
